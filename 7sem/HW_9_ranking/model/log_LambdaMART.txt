
[+] General Parameters:
Training data:	./data/train.txt
Test data:	./data/test.txt
Validation data:	./data/valid.txt
Feature vector representation: Dense.
Ranking method:	LambdaMART
Feature description file:	Unspecified. All features will be used.
Train metric:	NDCG@10
Test metric:	NDCG@10
Feature normalization: No
Model file: ./model/LambdaMART_100_5.txt

[+] LambdaMART's Parameters:
No. of trees: 100
No. of leaves: 5
No. of threshold candidates: 256
Learning rate: 0.1
Stop early: 100 rounds without performance gain on validation data

Reading feature file [./data/train.txt]: 0... Reading feature file [./data/train.txt]... [Done.]            
(938 ranked lists, 9847 entries read)
Reading feature file [./data/valid.txt]: 0... Reading feature file [./data/valid.txt]... [Done.]            
(964 ranked lists, 9726 entries read)
Reading feature file [./data/test.txt]: 0... Reading feature file [./data/test.txt]... [Done.]            
(937 ranked lists, 9619 entries read)
Initializing... [Done]
---------------------------------
Training starts...
---------------------------------
#iter   | NDCG@10-T | NDCG@10-V | 
---------------------------------
1       | 0.6639    | 0.68      | 
2       | 0.6674    | 0.6864    | 
3       | 0.6674    | 0.6866    | 
4       | 0.6677    | 0.6868    | 
5       | 0.6685    | 0.6859    | 
6       | 0.6682    | 0.6838    | 
7       | 0.6714    | 0.6878    | 
8       | 0.6715    | 0.6877    | 
9       | 0.674     | 0.6912    | 
10      | 0.6745    | 0.6933    | 
11      | 0.6744    | 0.6934    | 
12      | 0.6796    | 0.7002    | 
13      | 0.6795    | 0.7012    | 
14      | 0.6838    | 0.7019    | 
15      | 0.685     | 0.7029    | 
16      | 0.6866    | 0.7025    | 
17      | 0.6899    | 0.7069    | 
18      | 0.6927    | 0.7096    | 
19      | 0.6924    | 0.7097    | 
20      | 0.6962    | 0.7127    | 
21      | 0.6976    | 0.7133    | 
22      | 0.6978    | 0.7137    | 
23      | 0.6976    | 0.7161    | 
24      | 0.6983    | 0.7151    | 
25      | 0.6983    | 0.7143    | 
26      | 0.6972    | 0.7141    | 
27      | 0.6983    | 0.7147    | 
28      | 0.7003    | 0.7151    | 
29      | 0.7007    | 0.7157    | 
30      | 0.7018    | 0.7149    | 
31      | 0.7043    | 0.7158    | 
32      | 0.705     | 0.7166    | 
33      | 0.7054    | 0.7174    | 
34      | 0.7057    | 0.7169    | 
35      | 0.7076    | 0.7174    | 
36      | 0.7077    | 0.7172    | 
37      | 0.7094    | 0.7185    | 
38      | 0.7105    | 0.7194    | 
39      | 0.711     | 0.7194    | 
40      | 0.7117    | 0.7189    | 
41      | 0.7113    | 0.7191    | 
42      | 0.7123    | 0.7183    | 
43      | 0.7128    | 0.718     | 
44      | 0.713     | 0.7181    | 
45      | 0.7135    | 0.7188    | 
46      | 0.7133    | 0.7187    | 
47      | 0.7149    | 0.7195    | 
48      | 0.7149    | 0.7193    | 
49      | 0.7161    | 0.7203    | 
50      | 0.7156    | 0.7212    | 
51      | 0.7163    | 0.7212    | 
52      | 0.7188    | 0.722     | 
53      | 0.7191    | 0.7223    | 
54      | 0.7192    | 0.7232    | 
55      | 0.7196    | 0.7225    | 
56      | 0.7198    | 0.7225    | 
57      | 0.7197    | 0.7227    | 
58      | 0.7202    | 0.7235    | 
59      | 0.7204    | 0.7223    | 
60      | 0.7193    | 0.7226    | 
61      | 0.7203    | 0.723     | 
62      | 0.7208    | 0.7228    | 
63      | 0.7225    | 0.7229    | 
64      | 0.7227    | 0.7232    | 
65      | 0.7228    | 0.7227    | 
66      | 0.7235    | 0.7227    | 
67      | 0.7241    | 0.7228    | 
68      | 0.725     | 0.7227    | 
69      | 0.7254    | 0.7234    | 
70      | 0.7252    | 0.7229    | 
71      | 0.725     | 0.7236    | 
72      | 0.7262    | 0.7233    | 
73      | 0.7265    | 0.7235    | 
74      | 0.7275    | 0.7251    | 
75      | 0.7278    | 0.725     | 
76      | 0.7283    | 0.7266    | 
77      | 0.7291    | 0.7268    | 
78      | 0.7291    | 0.7256    | 
79      | 0.7302    | 0.7252    | 
80      | 0.7304    | 0.7247    | 
81      | 0.7303    | 0.724     | 
82      | 0.7304    | 0.7248    | 
83      | 0.7305    | 0.7238    | 
84      | 0.7306    | 0.7234    | 
85      | 0.7317    | 0.7237    | 
86      | 0.7316    | 0.724     | 
87      | 0.7321    | 0.724     | 
88      | 0.732     | 0.7248    | 
89      | 0.7329    | 0.7243    | 
90      | 0.7334    | 0.7242    | 
91      | 0.7335    | 0.7245    | 
92      | 0.7333    | 0.7247    | 
93      | 0.7335    | 0.7251    | 
94      | 0.7334    | 0.7248    | 
95      | 0.7333    | 0.7254    | 
96      | 0.7333    | 0.7247    | 
97      | 0.7335    | 0.7254    | 
98      | 0.7338    | 0.7255    | 
99      | 0.734     | 0.7252    | 
100     | 0.7338    | 0.726     | 
---------------------------------
Finished sucessfully.
NDCG@10 on training data: 0.7291
NDCG@10 on validation data: 0.7268
---------------------------------
NDCG@10 on test data: 0.6959

Model saved to: ./model/LambdaMART_100_5.txt
