{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [warning! If you run inside binder, do not forget to download. Your session will end in ~1h]\n",
    "\n",
    "# This tutorial is will bring you through your first deep reinforcement learning model\n",
    "\n",
    "\n",
    "* Seaquest game as an example\n",
    "* Training a simple lasagne neural network for Q_learning objective\n",
    "\n",
    "\n",
    "## About OpenAI Gym\n",
    "\n",
    "* Its a recently published platform that basicly allows you to train agents in a wide variety of environments with near-identical interface.\n",
    "* This is twice as awesome since now we don't need to write a new wrapper for every game\n",
    "* Go check it out!\n",
    "  * Blog post - https://openai.com/blog/openai-gym-beta/\n",
    "  * Github - https://github.com/openai/gym\n",
    "\n",
    "\n",
    "## New to Lasagne and AgentNet?\n",
    "* We only require surface level knowledge of theano and lasagne, so you can just learn them as you go.\n",
    "* Alternatively, you can find Lasagne tutorials here:\n",
    " * Official mnist example: http://lasagne.readthedocs.io/en/latest/user/tutorial.html\n",
    " * From scratch: https://github.com/ddtm/dl-course/tree/master/Seminar4\n",
    " * From theano: https://github.com/craffel/Lasagne-tutorial/blob/master/examples/tutorial.ipynb\n",
    "* This is pretty much the basic tutorial for AgentNet, so it's okay not to know it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "* Here we basically just load the game and check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global params.\n",
    "\n",
    "#game title. full list of games = http://yavar.naddaf.name/ale/list_of_current_games.html\n",
    "GAME=\"space_invaders\"\n",
    "\n",
    "#game image will be resized from (210,160) to your image_size. \n",
    "#You may want a bigger image for your homework assignment IF you want a larger NN\n",
    "IMAGE_W,IMAGE_H = IMAGE_SIZE =(105,80)\n",
    "\n",
    "#number of parallel agents and batch sequence length (frames)\n",
    "#For large scale training you may need more agents or experience replay for stability\n",
    "N_AGENTS = 10\n",
    "SEQ_LENGTH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/main/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=\"floatX=float32\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%env THEANO_FLAGS=\"floatX=float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f478609b090>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM8AAAD/CAYAAABW+4LyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f47861705d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from env import Atari\n",
    "\n",
    "#creating a game\n",
    "atari = Atari(GAME,image_size=IMAGE_SIZE) \n",
    "\n",
    "action_names = np.array(atari.get_action_meanings())\n",
    "\n",
    "obs = atari.step(0)[0]\n",
    "\n",
    "plt.imshow(obs,interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic agent setup\n",
    "Here we define a simple agent that maps game images into Qvalues using simple convolutional neural network.\n",
    "\n",
    "![scheme](https://s18.postimg.org/gbmsq6gmx/dqn_scheme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import InputLayer, DimshuffleLayer\n",
    "\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y,color)\n",
    "observation_layer = InputLayer((None,IMAGE_W,IMAGE_H,3))\n",
    "\n",
    "#reshape to [sample, color, x, y] to allow for convolutional layers to work correctly\n",
    "observation_reshape = DimshuffleLayer(observation_layer,(0,3,1,2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import Conv2DLayer,Pool2DLayer,DenseLayer,batch_norm,dropout\n",
    "\n",
    "#main neural network body\n",
    "#e.g. \n",
    "#Conv2DLayer(<prev layer>,n_filters,filter_size=(8,8),stride=(4,4),name='name')\n",
    "#DenseLayer(<prev layer,n units,nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "conv0 = Conv2DLayer(observation_reshape, 32, filter_size=(16,16), stride=(4,4), name='conv0')\n",
    "conv1 = Conv2DLayer(conv0, 32, filter_size=(8,8), stride=(2,2), name='conv1')\n",
    "dense0 = DenseLayer(conv1, 256, name='dense', nonlinearity=lasagne.nonlinearities.tanh)\n",
    "\n",
    "#load(dense0,\"./spaceinvaders_dqn_body_small.pcl\")\n",
    "\n",
    "#please set this to your last layer for convenience\n",
    "last_layer = dense0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a layer that predicts Qvalues for all actions.\n",
    "# Just adense layer with corresponding number of units and no nonlinearity (lasagne.nonlinearity.linear)\n",
    "n_actions = atari.action_space.n\n",
    "qvalues_layer = DenseLayer(last_layer, n_actions, nonlinearity=None)\n",
    "\n",
    "#To pick actions, we use an epsilon-greedy resolver (epsilon is a property)\n",
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer,name=\"e-greedy action picker\")\n",
    "\n",
    "action_layer.epsilon.set_value(np.float32(0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "#all together\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              policy_estimators=qvalues_layer,\n",
    "              action_layers=action_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[conv0.W, conv0.b, conv1.W, conv1.b, dense.W, dense.b, W, b]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(action_layer,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pool import AtariGamePool\n",
    "\n",
    "pool = AtariGamePool(agent,GAME, N_AGENTS,image_size=IMAGE_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE'\n",
      "  'LEFTFIRE']\n",
      " ['LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE'\n",
      "  'LEFTFIRE']\n",
      " ['LEFT' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'RIGHT' 'LEFTFIRE' 'LEFTFIRE']\n",
      " ['LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'RIGHT' 'LEFTFIRE' 'LEFTFIRE']\n",
      " ['LEFTFIRE' 'RIGHT' 'LEFTFIRE' 'LEFTFIRE' 'FIRE' 'LEFTFIRE' 'LEFTFIRE']\n",
      " ['LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFT']\n",
      " ['LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE'\n",
      "  'LEFTFIRE']\n",
      " ['LEFTFIRE' 'LEFT' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE']\n",
      " ['LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE'\n",
      "  'LEFTFIRE']\n",
      " ['LEFTFIRE' 'LEFTFIRE' 'RIGHT' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE' 'LEFTFIRE']]\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.]]\n",
      "CPU times: user 1.29 s, sys: 0 ns, total: 1.29 s\n",
      "Wall time: 1.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "_,action_log,reward_log,_,_,_  = pool.interact(7)\n",
    "\n",
    "\n",
    "print(action_names[action_log])\n",
    "print(reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load first sessions (this function calls interact and remembers sessions)\n",
    "pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "* An agent has a method that produces symbolic environment interaction sessions\n",
    "* Such sessions are in sequences of observations, agent memory, actions, q-values,etc\n",
    "  * one has to pre-define maximum session length.\n",
    "\n",
    "* SessionPool also stores rewards (Q-learning objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay.\n",
    "\n",
    "#This is an \"environment\" that replays all stored sessions.\n",
    "replay = pool.experience_replay\n",
    "\n",
    "#To only sample several random sessions, try\n",
    "#replay = pool.experience_replay.sample_session_batch(100,replace=True)\n",
    "\n",
    "_,_,_,_,qvalues_seq = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    optimize_experience_replay=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get reference Qvalues according to Qlearning algorithm\n",
    "from agentnet.learning import qlearning\n",
    "\n",
    "#loss for Qlearning = (Q(s,a) - (r+gamma*Q(s',a_max)))^2\n",
    "\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                      replay.actions,\n",
    "                                                      replay.rewards,\n",
    "                                                      replay.is_alive,\n",
    "                                                      gamma_or_gammas=0.99,)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute weight updates. Replace with any optimizer you want\n",
    "updates = lasagne.updates.adam(loss,weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#compile train function\n",
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-19 08:30:17,487] Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\n",
      "[2016-10-19 08:30:17,489] Creating monitor directory ./records\n",
      "[2016-10-19 08:30:17,530] Starting new video recorder writing to /home/main/notebooks/records/openaigym.video.0.13.video000000.mp4\n",
      "[2016-10-19 08:30:51,740] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/main/notebooks/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 1208 timesteps with reward=210.0\n"
     ]
    }
   ],
   "source": [
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./records/openaigym.video.0.13.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_path = \"./records/openaigym.video.0.13.video000000.mp4\"\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_epsilon(epoch_counter):\n",
    "    \"\"\"\n",
    "    a function which outputs current epsilon for e-greedy exploration given training iteration.\n",
    "    \"\"\"\n",
    "    <implement me!>\n",
    "    return 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "\n",
    "#full game rewards\n",
    "rewards = {epoch_counter:untrained_reward}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#the loop may take eons to finish.\n",
    "#consider interrupting early.\n",
    "for i in xrange(10**10):    \n",
    "    \n",
    "    \n",
    "    #train\n",
    "    pool.update(SEQ_LENGTH)\n",
    "    loss = train_step()\n",
    "    \n",
    "    ##update resolver's epsilon (chance of random action instead of optimal one)\n",
    "    current_epsilon = get_epsilon(epoch_counter)\n",
    "    action_layer.epsilon.set_value(np.float32(current_epsilon))\n",
    "    \n",
    "    if epoch_counter%10==0:\n",
    "        print(\"iter=%i\\tepsilon=%.3f\"%(epoch_counter,current_epsilon))\n",
    "    \n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%100 ==0:\n",
    "        rewards[epoch_counter] = pool.evaluate(record_video=False)\n",
    "        \n",
    "        plt.title(\"random frames\")\n",
    "        for i in range(min((len(pool.games),6))):\n",
    "            plt.subplot(2,3,i+1)\n",
    "            plt.imshow(pool.games[i].get_observation())\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results\n",
    " * Here we plot learning curves and sample testimonials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(*zip(*sorted(list(rewards.items()),key=lambda p:p[0])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rw = pool.evaluate(n_games=20,save_path=\"./records\",record_video=True)\n",
    "print \"mean session score=%f.5\"%rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "#select the one you want\n",
    "video_path= <choose video name from what previous cell says>\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(video_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Pre-trained net.\n",
    "\n",
    "To speed up training, consider using the pre-trained network body and only fine-tuning the qValues layers.\n",
    "\n",
    "Below you can find a function that loads such pre-trained network. You can use it to initialize the Q-network body.\n",
    "\n",
    "It may make sense to not train the pre-trained part of body for a few iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from agentnet.utils.persistence import save,load\n",
    "def build_pretrained_spaceinvaders(observation_reshape):\n",
    "    \"\"\" a smaller version of DQN pre-trained for ~2 hours on GPU \"\"\"\n",
    "    assert tuple(observation_reshape.output_shape) == (None, 3, 105, 80)\n",
    "    \n",
    "    #main neural network body\n",
    "    conv0 = Conv2DLayer(observation_reshape,16,filter_size=(8,8),stride=(4,4),name='conv0')\n",
    "    conv1 = Conv2DLayer(conv0,32,filter_size=(4,4),stride=(2,2),name='conv1')\n",
    "    dense0 = DenseLayer(conv1,256,name='dense',nonlinearity=lasagne.nonlinearities.tanh)\n",
    "    \n",
    "    load(dense0,\"./spaceinvaders_dqn_body_small.pcl\")\n",
    "    return dense0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Once you got it working,\n",
    "Try building a network that maximizes the final score\n",
    "\n",
    "* Moar lasagne stuff: convolutional layers, batch normalization, nonlinearities and so on\n",
    "* Recurrent agent memory layers, GRUMemoryLayer, etc\n",
    "* Different reinforcement learning algorithm (p.e. qlearning_n_step), other parameters\n",
    "* Experience replay pool\n",
    "\n",
    "\n",
    "Look for info?\n",
    "* [lasagne doc](http://lasagne.readthedocs.io/en/latest/)\n",
    "* [agentnet doc](http://agentnet.readthedocs.io/en/latest/)\n",
    "* [gym homepage](http://gym.openai.com/)\n",
    "\n",
    "\n",
    "You can also try to expand to a different game: \n",
    " * all OpenAI Atari games are already compatible, you only need to change GAME_TITLE\n",
    " * Other discrete action space environments are also accessible this way\n",
    " * For continuous action spaces, either discretize actions or use continuous RL algorithms (e.g. .learning.dpg_n_step)\n",
    " * Adapting to a custom non-OpenAI environment can be done with a simple wrapper\n",
    " \n",
    " \n",
    "__Good luck!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
